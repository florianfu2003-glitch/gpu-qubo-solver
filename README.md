# GPU-Accelerated QUBO Brute-Force Solver  
### CUDA ‚Ä¢ C++ ‚Ä¢ HPC ‚Ä¢ Dense & Sparse Matrices ‚Ä¢ Gray-Code Incremental Update

This project implements a **massively parallel brute-force solver** for  
**QUBO (Quadratic Unconstrained Binary Optimization)** problems using both  
CPU-based and **GPU-accelerated** approaches.

The focus is on **incremental energy evaluation** via **Gray-code traversal**,  
allowing high-performance enumeration of all \(2^n\) binary states for  
problems up to **63 variables**.

This work was completed as part of the  
_Programmierung Massiv-Paralleler Prozessoren (PMPP)_ course at  
**TU Darmstadt**, and further extended into a standalone research-grade system.

---

## Key Features

### GPU Acceleration

- Custom CUDA kernels for dense and sparse (CSR) QUBO matrices  
- Efficient memory access patterns  
- Fully device-side Gray-code bit-flip traversal  
- Incremental energy update reduces cost from `O(n¬≤)` to `O(n)` per step  

### CPU Implementations

- Naive brute-force solver (full recomputation of the energy)
- Optimized CPU solver using incremental energy update
- OpenMP parallelization with dynamic partitioning

### Matrix Support

- `DenseMatrix` ‚Äî row-major dense QUBO matrices  
- `SparseMatrix` ‚Äî CSR-based storage for large sparse QUBOs  

### Performance

- Achieves **up to 20‚Äì50√ó speedup** over the optimized CPU implementation  
- Designed for HPC systems (tested on Lichtenberg Cluster @ TU Darmstadt)

---

## Background

A QUBO problem minimizes the quadratic binary energy function

**E(x) = x·µÄ Q x**, with **x ‚àà {0,1}‚Åø**.

Brute-force search over all `2‚Åø` states becomes quickly infeasible, but  
for `n ‚â§ 30` GPUs can evaluate millions of states in parallel.

To avoid recomputing the energy from scratch for every state, we use:

### Gray-Code Traversal

We iterate over all binary states in Gray-code order, so that only **one bit changes** between consecutive states:

- Let `x‚Çñ` be the current state.
- The next state is `x‚Çñ‚Çä‚ÇÅ = x‚Çñ ‚äï (1 << ctz(k + 1))`,  
  where `ctz` is the count of trailing zeros.

This is implemented via `std::countr_zero` on the CPU and `__ffsll` on the GPU.

### Row-Flip Incremental Update

When bit `i` flips, the energy difference can be computed as:

**ŒîE = Œ£‚±º Q·µ¢‚±º x‚±º + Q·µ¢·µ¢ (1 - x·µ¢) - Q·µ¢·µ¢ x·µ¢**

so the new energy is just `E_new = E_old + ŒîE`.

This incremental update is used in both CPU and GPU variants and reduces the
per-state cost from `O(n¬≤)` to `O(n)`.

---

## Repository Structure

```text
gpu-qubo-solver/
‚îÇ
‚îú‚îÄ‚îÄ CMakeLists.txt           # Top-level CMake build script
‚îú‚îÄ‚îÄ README.md                # Project description
‚îú‚îÄ‚îÄ run.sh                   # Optional helper script (e.g. SLURM job on a cluster)
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ CMakeLists.txt       # CMake configuration for the library/executable
‚îÇ   ‚îú‚îÄ‚îÄ main.cpp             # Entry point (CPU & GPU comparison and timing)
‚îÇ   ‚îú‚îÄ‚îÄ cpu_brute_force.h    # Naive + incremental CPU brute-force solvers
‚îÇ   ‚îú‚îÄ‚îÄ gpu_brute_force.cu   # CUDA kernels + GPU solver implementation
‚îÇ   ‚îú‚îÄ‚îÄ gpu_brute_force.h
‚îÇ   ‚îú‚îÄ‚îÄ qubo_energy.h        # Dense & sparse energy + incremental ŒîE
‚îÇ   ‚îú‚îÄ‚îÄ matrix.h             # Dense / sparse matrix representations
‚îÇ   ‚îú‚îÄ‚îÄ matrix_reader.h      # MatrixMarket (.mtx) loader
‚îÇ   ‚îú‚îÄ‚îÄ state_vector.h       # Bitset ‚Üî vector conversions
‚îÇ   ‚îú‚îÄ‚îÄ datatypes.h          # Type aliases used throughout the project
‚îÇ   ‚îú‚îÄ‚îÄ cuda_util.h          # CUDA error handling helpers
‚îÇ   ‚îú‚îÄ‚îÄ cuda_timer.h         # Simple GPU timing utilities
‚îÇ   ‚îú‚îÄ‚îÄ cuda_debug.h
‚îÇ   ‚îî‚îÄ‚îÄ qubo_brute_forcer.h  # Base class interface for CPU/GPU solvers
```

---

## ‚öôÔ∏è Build Instructions

### Requirements

- CUDA Toolkit ‚â• 12.x  
- GCC ‚â• 11 or MSVC ‚â• 19  
- CMake ‚â• 3.20  
- Optional: OpenMP-enabled compiler  
- Linux or Windows (also tested under WSL)

---

### Build (Linux / WSL / Cluster Login Node)

```bash
mkdir build
cd build
cmake ..
make -j8
```

This produces an executable `QUBOBruteForcing` inside `build/`.

On HPC systems (e.g., Lichtenberg Cluster), load modules first:

```bash
module load cuda/12.5 gcc/13.1.0 cmake
```

---

## ‚ñ∂Ô∏è Run

After building, run the solver with a MatrixMarket `.mtx` file:

```bash
./QUBOBruteForcing path/to/matrix.mtx
```

Example:

```bash
./QUBOBruteForcing ../data/block_encoding_20.mtx
```

Typical output:

```text
Matrix: block_encoding_20.mtx
20 x 20, nnz = 60

CPU (sparse):
Elapsed time: 15.23 ms

GPU (sparse):
Elapsed time: 0.31 ms
Speedup: 49.1x

Optimal states:
0: 1 0 1 0 1 1 0 0 1 1 ... Energy: -12.0
```

On clusters, you can use the included `run.sh` for SLURM submission.

---

## üî¨ Technical Overview

### Parallelization Strategy

- The state space is partitioned across `2·µè` GPU threads.
- Each thread enumerates a contiguous subspace of the full search space.
- Gray-code ordering ensures only **one bit flips** at each step.
- Bit index computed by:
  - `std::countr_zero(k+1)` on CPU  
  - `__ffsll(k+1) - 1` on GPU  
- Energy updates are computed incrementally:

```text
E_new = E_old + ŒîE
```

- Complexity reduced from `O(n¬≤¬∑2‚Åø)` ‚Üí `O(n¬∑2‚Åø)`.

---

### Dense vs Sparse Handling

| Matrix Type | Storage | Kernel Characteristics |
|-------------|---------|------------------------|
| Dense       | Row-major `Q[n][n]` | Good for small/medium n; simple memory access |
| Sparse      | CSR (`values`, `offsets`, `columns`) | Skips zero entries; fast incremental update |

Both formats support up to **63 variables** (due to 64-bit encoding).

---

## ‚è± Performance Overview

Benchmarks on NVIDIA Tesla T4 (Lichtenberg Cluster):

| Matrix Size (n) | CPU Naive | CPU Incremental | GPU | Speedup |
|-----------------|-----------|------------------|------|---------|
| 20              | 120 ms    | 15 ms            | 0.31 ms | 48√ó |
| 25              | ‚Äî         | 490 ms           | 6.1 ms  | 80√ó |
| 28              | ‚Äî         | 2600 ms          | 21 ms   | 120√ó |

Results depend on sparsity structure and GPU architecture.

---

## ‚ö†Ô∏è Limitations

- Maximum variables: **63** (due to 64-bit state representation).  
- Dense GPU kernel uses `O(n¬≤)` memory.  
- QUBOs with `n > 32` may still require significant runtime.  
- Sparse performance varies with matrix structure.

---

## üöß Future Work

- Multi-GPU parallelization  
- Shared-memory optimized kernels  
- ELLPACK / sliced-ELL sparse formats  
- Heuristic solvers (SA, tabu search, evolutionary methods)  
- Hybrid CPU/GPU enumeration  
- Python bindings (PyBind11)

---

## üìÑ License

Released under the **MIT License**.  
Template available at:  
https://opensource.org/licenses/MIT

---

## üôè Acknowledgements

This project was developed as part of:

> PMPP ‚Äì Programmierung Massiv-Paralleler Prozessoren  
> Technische Universit√§t Darmstadt (WS 2025/26)

and expanded into an independent research-oriented system.

---

## üë§ Author

**Bo Fu**  
M.Sc. Informatik, TU Darmstadt  
GPU Computing ‚Ä¢ Optimization ‚Ä¢ High-Performance Computing  

GitHub: https://github.com/florianfu2003-glitch
